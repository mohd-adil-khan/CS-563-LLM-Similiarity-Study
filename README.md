# semantic_similarity
Here's a README file based on the content provided:

---

# Comparative Analysis of Large Language Models (LLMs) Based on Embedding Similarity

## Introduction

This project presents a comparative analysis of two Large Language Models (LLMs)—RoBERTa and T5—focusing on their semantic similarity at the embedding level. As LLMs have become integral to Natural Language Processing (NLP) tasks, evaluating their capabilities in understanding and generating language with semantic depth is crucial. This project introduces a novel approach to comparing these models, using Fill-in-the-Blanks and Extractive Question Answering tasks as evaluation benchmarks.

## Problem Statement

The primary challenge addressed by this project is the measurement of semantic similarity between LLMs. Traditional metrics, such as the F1 score, often fall short in capturing the nuanced semantic similarities between model outputs. This project proposes a method to evaluate the semantic content of the models' responses, moving beyond simple lexical accuracy.

## Importance

The findings from this analysis have several key implications:

- **Understanding Model Capabilities**: The Fill-in-the-Blanks task provides insights into the models' ability to infer context and generate coherent responses.
- **Quality Assessment**: By evaluating the quality of generated text, this project aids in ensuring the reliability of LLMs in real-world applications.
- **Improving Model Performance**: Analyzing the similarities and differences between model outputs can guide future improvements in LLM architecture and training strategies.
- **Application to Downstream Tasks**: The results from this project can inform the selection of models for specific NLP tasks, such as question answering and text summarization.

## Methodology

### Cloze Filling Task

- **Data Collection**: A dataset was curated from the "SimpleBooks" corpus, consisting of simplified English text.
- **Model Implementation**: RoBERTa and T5 were used to generate answers for fill-in-the-blank questions.
- **SemScore Analysis**: The semantic similarity between model responses was evaluated using SemScore, a cosine similarity metric based on sentence embeddings.

### Extractive Question Answering Task

- **Data**: The SQuAD 2.0 dataset was used, containing context-question-answer pairs.
- **Evaluation**: Models were evaluated using F1 score, Exact Matches (EM), and SemScore.

## Evaluation

The evaluation involved two key tasks:

1. **Cloze Filling Task**: The similarity between the responses generated by RoBERTa and T5 was measured, revealing that despite their architectural differences, the models exhibited a high degree of semantic similarity.
   
2. **Extractive Question Answering Task**: The performance of RoBERTa and T5 was compared using F1 score, EM, and SemScore. Results indicated that while T5 often provided more exact matches, both models had a similar distribution of high and low-quality responses.

## Limitations

Several limitations were identified:

- **Multi-word Answers**: Larger models like T5 may generate multi-word answers, complicating cosine similarity comparisons.
- **Context Dependency**: The sequence of tasks can influence model responses, affecting accuracy.
- **Bias in Evaluation**: Differences in model architecture, training data, and fine-tuning may bias the results, making it difficult to standardize comparisons.

## Future Work

Future research could explore other methods to quantify model similarity, such as comparing model architectures, training data, hyperparameters, and fine-tuning processes. Additionally, the critical judgment of models based on the corpus they generate is an unexplored domain that warrants further investigation.

